<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Vrecan...</title>
    <link>http://vrecan.github.io/post/</link>
    <description>Recent content in Posts on Vrecan...</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Dec 2015 16:38:08 -0700</lastBuildDate>
    <atom:link href="http://vrecan.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>InfluxDB Terminal Dashboards with FluxDash</title>
      <link>http://vrecan.github.io/post/intro_fluxdash/</link>
      <pubDate>Mon, 28 Dec 2015 16:38:08 -0700</pubDate>
      
      <guid>http://vrecan.github.io/post/intro_fluxdash/</guid>
      <description>

&lt;h3 id=&#34;overview:890ca4461a7712ff7ba32a852ca76ffd&#34;&gt;Overview&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vrecan/FluxDash&#34;&gt;FluxDash&lt;/a&gt; is a dashboard terminal ui that is fully configurable through json files. You can either give it an indivudual json file or a folder and it will read any json file in the folder and attemp to turn it into a dashboard. You can then move between dashboard by pressing &lt;N&gt;.&lt;/p&gt;

&lt;h3 id=&#34;features:890ca4461a7712ff7ba32a852ca76ffd&#34;&gt;Features&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Sparklines&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sparklines are a simplified single stat linechart that gives you basic trending over time.
Example json:&lt;/p&gt;

&lt;pre&gt;
    &lt;code class=&#34;json-css&#34;&gt;
&#34;sparklines&#34;: {
    &#34;Height&#34;: 1,
    &#34;lines&#34;: [{
        &#34;from&#34;: &#34;/system.cpu/&#34;,
        &#34;title&#34;: &#34;CPU&#34;,
        &#34;where&#34;: &#34;&#34;, 
        &#34;type&#34;: 2,

        &#34;height&#34;: 1,
        &#34;linecolor&#34;: 5,
        &#34;titlecolor&#34;: 0
    }, {
        &#34;from&#34;: &#34;/system.mem.free/&#34;,
        &#34;title&#34;: &#34;System Free memory&#34;,
        &#34;type&#34;: 3,
        &#34;height&#34;: 1
    }, {
        &#34;from&#34;: &#34;/system.mem.cached/&#34;,
        &#34;title&#34;: &#34;System Cached memory&#34;,
        &#34;type&#34;: 3,
        &#34;height&#34;: 1
    }, {
        &#34;from&#34;: &#34;/system.mem.buffers/&#34;,
        &#34;title&#34;: &#34;System Buffers memory&#34;,
        &#34;type&#34;: 3,
        &#34;height&#34;: 1
    }]
}
&lt;/code&gt;
&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;MultiSpark&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A set of sparklines that have been expanded automatically based on a
influxdb tag.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Gauge&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gauge shows you the mean and max over the set time period.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BarChart&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;BarChart is a graph that represents grouped data.&lt;/p&gt;

&lt;h3 id=&#34;json-definition:890ca4461a7712ff7ba32a852ca76ffd&#34;&gt;Json Definition&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Types:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;
    &lt;code class=&#34;go-css&#34;&gt;const(     
    Short   = 1 #human readable numbers 100 1k 10m 1b
    Percent = 2 #Data should be treated as a percent
    Bytes   = 3 #human readable bytes 1b 30kb 1mb 10gb 1tb
    Time    = 4 #human readable time duration 1sec 5m 1h
)
&lt;/code&gt;
&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;From:
From is the measurement you are going to query in influxdb.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;ldquo;&lt;code&gt;/system.cpu.*/&lt;/code&gt;&amp;rdquo; example regex.&lt;br&gt;
&amp;ldquo;&lt;code&gt;system.cpu&lt;/code&gt;&amp;rdquo; example full measurement name.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Where:
Where allows you to add to the where clause in your influxdb query.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;ldquo;&lt;code&gt;service=&#39;name&#39;&lt;/code&gt;&amp;ldquo;&lt;br&gt;
&amp;ldquo;`service=&amp;lsquo;name&amp;rsquo; AND host=&amp;lsquo;hostname&amp;rsquo;&amp;rdquo;&lt;/p&gt;

&lt;h3 id=&#34;example-dashboard:890ca4461a7712ff7ba32a852ca76ffd&#34;&gt;Example Dashboard&lt;/h3&gt;

&lt;p&gt;Here is an example dashboard. In the image below you will see that I am adjusting the time range of the queries for the dashboard. This is accomplished by pressing &lt;t&gt; to move forward in time and &lt;y&gt; to move back through the list of time ranges. Right now time ranges are hard coded to be:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;{&amp;quot;15m&amp;quot;, &amp;quot;30m&amp;quot;, &amp;quot;1h&amp;quot;, &amp;quot;8h&amp;quot;, &amp;quot;24h&amp;quot;, &amp;quot;2d&amp;quot;, &amp;quot;5d&amp;quot;, &amp;quot;7d&amp;quot;}&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This will become configurable eventually but these are the most commonly used in our system.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vrecan.github.io/fluxdashv2.gif&#34; alt=&#34;Example Dashboard&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;You will also notice that the refresh and the group by interval are adjusted based on the time range. Again this will be come configurable at a later date.&lt;/p&gt;

&lt;pre&gt;
    &lt;code class=&#34;json-css&#34;&gt;

{
    &#34;rows&#34;: [{
        &#34;height&#34;: 1,
        &#34;span&#34;: 12,
        &#34;offset&#34;: 0,
        &#34;columns&#34;: [{
            &#34;height&#34;: 1,
            &#34;span&#34;: 6,
            &#34;offset&#34;: 0,
            &#34;timep&#34;: {
                &#34;height&#34;: 3,
                &#34;border&#34;: true
            }
        },{
            &#34;height&#34;: 1,
            &#34;span&#34;: 3,
            &#34;offset&#34;: 0,
            &#34;p&#34;: {
                &#34;height&#34;: 3,
                &#34;text&#34;: &#34;System Health&#34;,
                &#34;border&#34;: true
            }
        },
        {
            &#34;height&#34;: 1,
            &#34;span&#34;: 3,
            &#34;offset&#34;: 0,
            &#34;loading&#34;: {
                &#34;height&#34;: 3,
                &#34;border&#34;: true,
                &#34;bordertop&#34;: true,
                &#34;borderbottom&#34;: true,
                &#34;borderright&#34;: true,
                &#34;borderleft&#34;: true,
                &#34;BarColor&#34;: 5,
                &#34;BorderFg&#34;: 5
            }
        }]
    },
     {
        &#34;height&#34;: 1,
        &#34;span&#34;: 12,
        &#34;offset&#34;: 0,
        &#34;columns&#34;: [{
            &#34;height&#34;: 1,
            &#34;span&#34;: 12,
            &#34;offset&#34;: 0,
            &#34;sparklines&#34;: {
                &#34;Height&#34; : 1,
                &#34;lines&#34;: [{
                    &#34;from&#34;: &#34;/system.cpu/&#34;,
                    &#34;title&#34;: &#34;CPU&#34;,
                    &#34;where&#34;: &#34;&#34;,
                    &#34;type&#34;: 2,
                    &#34;height&#34;: 1,
                    &#34;linecolor&#34; : 5,
                    &#34;titlecolor&#34;: 0
                }, {
                    &#34;from&#34;: &#34;/system.mem.free/&#34;,
                    &#34;title&#34;: &#34;System Free memory&#34;,
                    &#34;type&#34;: 3,
                    &#34;height&#34;: 1
                },
                {
                    &#34;from&#34;: &#34;/system.mem.cached/&#34;,
                    &#34;title&#34;: &#34;System Cached memory&#34;,
                    &#34;type&#34;: 3,
                    &#34;height&#34;: 1
                },{
                    &#34;from&#34;: &#34;/system.mem.buffers/&#34;,
                    &#34;title&#34;: &#34;System Buffers memory&#34;,
                    &#34;type&#34;: 3,
                    &#34;height&#34;: 1
                }]
            }
        }]
    }, 
     {
        &#34;height&#34;: 1,
        &#34;span&#34;: 12,
        &#34;offset&#34;: 0,
        &#34;columns&#34;: [{
            &#34;height&#34;: 1,
            &#34;span&#34;: 3,
            &#34;offset&#34;: 0,
            &#34;barchart&#34;: {
                &#34;height&#34;: 6,
                &#34;from&#34;: &#34;/es.indices/&#34;,
                &#34;where&#34;: &#34;service=&#39;gomaintain&#39;&#34;,
                &#34;borderLabel&#34;: &#34;Elastic Indices&#34;
            }
        },{
        &#34;height&#34;: 1,
        &#34;span&#34;: 6,
        &#34;offset&#34;: 0,
        &#34;gauge&#34;: {
            &#34;height&#34;: 3,
            &#34;from&#34;: &#34;/es.*.FS.Used.Percent/&#34;,
            &#34;where&#34;: &#34;service=&#39;gomaintain&#39;&#34;,
            &#34;border&#34;: true,
            &#34;BorderLabel&#34;: &#34;ES&#34;,
            &#34;bordertop&#34;: true,
            &#34;borderbottom&#34;: true,
            &#34;borderright&#34;: true,
            &#34;borderleft&#34;: true,
            &#34;BarColor&#34;: 5,
            &#34;BorderBg&#34;: 5,
            &#34;label&#34;: &#34;Disk Used&#34;
        }
    }]
},{
        &#34;height&#34;: 1,
        &#34;span&#34;: 12,
        &#34;offset&#34;: 0,
        &#34;columns&#34;: [{
            &#34;height&#34;: 1,
            &#34;span&#34;: 12,
            &#34;offset&#34;: 0,
            &#34;sparklines&#34;: {
                &#34;Height&#34; : 1,
                &#34;lines&#34;: [{
                    &#34;from&#34;: &#34;/system.disk.[^dm|util].*.read.bytes/&#34;,
                    &#34;title&#34;: &#34;Total Disk IO READ Bytes&#34;,
                    &#34;where&#34;: &#34;&#34;,
                    &#34;type&#34;: 3,
                    &#34;height&#34;: 1,
                    &#34;linecolor&#34; : 5,
                    &#34;titlecolor&#34;: 0
                },{
                    &#34;from&#34;: &#34;/system.disk.[^dm|util].*.write.bytes/&#34;,
                    &#34;title&#34;: &#34;Total Disk IO Write Bytes&#34;,
                    &#34;where&#34;: &#34;&#34;,
                    &#34;type&#34;: 3,
                    &#34;height&#34;: 1,
                    &#34;linecolor&#34; : 5,
                    &#34;titlecolor&#34;: 0
                }]
            }
        }]
    },
         {
        &#34;height&#34;: 1,
        &#34;span&#34;: 12,
        &#34;offset&#34;: 0,
        &#34;columns&#34;: [{
            &#34;height&#34;: 1,
            &#34;span&#34;: 12,
            &#34;offset&#34;: 0,
            &#34;multispark&#34;: {
                &#34;height&#34;: 3,
                &#34;from&#34;: &#34;/Dispatch.*/&#34;,
                &#34;where&#34;: &#34;service=&#39;godispatch&#39;&#34;,
                &#34;borderLabel&#34;: &#34;Dispatch&#34;,
                &#34;border&#34; : true,
                &#34;Bordertop&#34; : true,
                &#34;Borderbottom&#34; : true,
                &#34;Borderleft&#34; : true,
                &#34;Borderright&#34; : true,
                &#34;Borderfg&#34;: 4,              
                &#34;bg&#34; : 1,
                &#34;type&#34;: 1,
                &#34;autocolor&#34;: true,
                &#34;titlecolor&#34;: 6
            }
        }]
    }]
}
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vrecan/FluxDash&#34;&gt;Link to code&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gracefull Shutdown of Goroutines</title>
      <link>http://vrecan.github.io/post/graceful_goroutnes/</link>
      <pubDate>Sun, 29 Mar 2015 16:38:08 -0700</pubDate>
      
      <guid>http://vrecan.github.io/post/graceful_goroutnes/</guid>
      <description>

&lt;h3 id=&#34;overview:ec207a846908786b018fd316c62fe3cf&#34;&gt;Overview&lt;/h3&gt;

&lt;p&gt;A common problem people new to Go tend to run into is managing goroutines. Starting a goroutine is easy but how do you guarantee it closes gracefully when you are ready to exit? All code for this example lives &lt;a href=&#34;https://github.com/vrecan/shutdown_example&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, lets create a struct that will manage our goroutine. In this case it&amp;rsquo;s just going to be one but there are no restrictions here.&lt;/p&gt;

&lt;pre&gt;
    &lt;code class=&#34;go-css&#34;&gt;

//App is an example struct that will run goroutines
type App struct {
    wg   *sync.WaitGroup //allow us to wait for close
    done chan bool
    Data chan string
}

func NewApp() (app *App) {
    app = &amp;App{
        wg:   &amp;sync.WaitGroup{},
        done: make(chan bool, 1),
        Data: make(chan string, 100),
    }
    return app
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;With this we can now add a few methods to start and run our goroutine. First, our start method which will increment our wait group and run our goroutine. It&amp;rsquo;s important to increment the wait group before you spin up the goroutine otherwise you can have a race condition on close.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Start go routines
func (a *App) Start() {
	a.wg.Add(1)
	go a.runSelect()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are two main ways to handle signaling for shutdown. This example shows both for completeness. The first thing we are doing is using a channel called done in a select statement which allows us to get a signal from something external telling us it&amp;rsquo;s time for shutdown.&lt;/p&gt;

&lt;p&gt;The other way of managing shutdown of a goroutine is to just close it when you are done sending things to the channel. The benefit of this approach is that you will always process all of the messages before you exit. Obviously this could be a negative as well if you don&amp;rsquo;t want to process everything on the queue before a shutdown.&lt;/p&gt;

&lt;pre&gt;
    &lt;code class=&#34;go-css&#34;&gt;
//Run with select statement to manage shutdown
func (a *App) runSelect() {
    defer a.wg.Done()
loop:
    for {
        select {

        case &lt;-a.done:
            fmt.Println(&#34;Detected close doing cleanup and exiting&#34;)
            a.wg.Add(1)
            a.runClose()
            break loop
            //do cleanup if needed
        case s, ok := &lt;-a.Data:
            if ok { //ok will tell us if the channel has been shutdown
                fmt.Println(&#34;select: &#34;, s)
            } else {
                //done processing all messages exit
                fmt.Println(&#34;Breaking loop&#34;)
                break loop
            }
        }
    }
}
//process all messaging on the queue until we get close message
func (a *App) runClose() {
    for s := range a.Data {
        fmt.Println(&#34;close: &#34;, s)
    }
}
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Now it&amp;rsquo;s time for our simple close method that will send our done signal and
also close our channel. &lt;strong&gt;&lt;em&gt;In a real application you want to make sure to only close a Go channel after you can guarantee all the senders are done.&lt;/em&gt;&lt;/strong&gt; Go channels will panic if you try to send to a closed channel.&lt;/p&gt;

&lt;pre&gt;
    &lt;code class=&#34;go-css&#34;&gt;
//Cleanly close background routines
func (a *App) Close() {
    if a != nil {
        a.done &lt;- true
        close(a.Data) //this lets the recv know there is no more data
        a.wg.Wait()
    }
}
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Example main loop for completeness.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Simple example on managing shutdown
func main() {
	death := DEATH.NewDeath(SYS.SIGINT, SYS.SIGTERM)
	app := NewApp()
	app.Start()
	// app.Start()
	for i := 1; i &amp;lt;= 100; i++ {
		app.Data &amp;lt;- fmt.Sprintf(&amp;quot;TESTING %d&amp;quot;, i)
	}
	//use death
	death.WaitForDeath(app)
	//or call close manually
	//app.Close()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;conclusion:ec207a846908786b018fd316c62fe3cf&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I hope this example was useful. Every new Go developer I train seems to get stuck on managing closing goroutines and this information has been helpful in explaining the pitfalls of common Go patterns to shutdown.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vrecan/shutdown_example&#34;&gt;Link to code&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parallel Graceful Shutdown in Go</title>
      <link>http://vrecan.github.io/post/death_parallel_shutdown/</link>
      <pubDate>Thu, 19 Mar 2015 22:11:18 -0600</pubDate>
      
      <guid>http://vrecan.github.io/post/death_parallel_shutdown/</guid>
      <description>

&lt;h3 id=&#34;overview:d43027cab780c369fd9e2e145dc0c7e6&#34;&gt;Overview&lt;/h3&gt;

&lt;p&gt;After using &lt;a href=&#34;http://github.com/vrecan/death&#34;&gt;death&lt;/a&gt; for awhile I started to run into issues closing objects. Mainly that some poorly written code would never close and the app would hang and require me to force kill the app. This seemed like a perfect oppurtunity to use go routines and channels to solve a relatively complex threading problem. What I really want is parallel shutdown with an overall timeout that will shutdown even if something fails.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://github.com/vrecan/death&#34; title=&#34;Application shutdown library for golang&#34;&gt;Link to github project&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;how-does-it-work:d43027cab780c369fd9e2e145dc0c7e6&#34;&gt;How does it work?&lt;/h3&gt;

&lt;p&gt;So lets look at the new Wait for death method&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Wait for death and then kill all items that need to die.
func (d *Death) WaitForDeath(closable ...Closable) {
	d.wg.Wait()
	log.Info(&amp;quot;Shutdown started...&amp;quot;)
	count := len(closable)
	log.Debug(&amp;quot;Closing &amp;quot;, count, &amp;quot; objects&amp;quot;)
	if count &amp;gt; 0 {
		d.closeInMass(closable...)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What you will see here is that now instead of calling close we are now calling this close in mass method.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Close all the objects at once and wait forr them to finish with a channel.
func (d *Death) closeInMass(closable ...Closable) {
	count := len(closable)
	//call close async
	done := make(chan bool, count)

	for _, c := range closable {
		go d.closeObjects(c, done)
	}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Close in mass is going to get the size of the slice and then make a channel to receive done messages. This channel will then be passed to a goroutine that sends a message when close is finished.&lt;/p&gt;

&lt;p&gt;Here is what the function looks like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Close objects and return a bool when finished on a channel.
func (d *Death) closeObjects(c Closable, done chan&amp;lt;- bool) {
	c.Close()
	done &amp;lt;- true
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we use a timer and a select statement to wait for events on the timer channel and the done channel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	//wait on channel for notifications.

	timer := time.NewTimer(d.timeout)
	for {
		select {
		case &amp;lt;-timer.C:
			log.Warn(count, &amp;quot; object(s) remaining but timer expired.&amp;quot;)
			return
		case &amp;lt;-done:
			count--
			log.Debug(count, &amp;quot; object(s) left&amp;quot;)
			if count == 0 {
				log.Debug(&amp;quot;Finished closing objects&amp;quot;)
				return
			}
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will block until our timer event is triggered or the number of goroutines running equals 0.&lt;/p&gt;

&lt;h1 id=&#34;summary:d43027cab780c369fd9e2e145dc0c7e6&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;With this change my applications have seen a 2-3x shutdown speed increase with the benefit of always shutting down even if a bad objects close method never returns.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://github.com/vrecan/death&#34; title=&#34;Application shutdown library for golang&#34;&gt;Managing death in go&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing Application Shutdown in Go</title>
      <link>http://vrecan.github.io/post/golang_shutdown/</link>
      <pubDate>Tue, 10 Mar 2015 20:23:05 -0600</pubDate>
      
      <guid>http://vrecan.github.io/post/golang_shutdown/</guid>
      <description>

&lt;h3 id=&#34;overview:a3f014005cf18d6d375bffc268e2748e&#34;&gt;Overview&lt;/h3&gt;

&lt;p&gt;I was looking around and couldn&amp;rsquo;t find a simple library that would manage application shutdown using signals. I decided to build a simple library to handle application shutdown and calling Close methods on structs when shutdown was signaled. It is called &lt;a href=&#34;http://github.com/vrecan/death&#34; title=&#34;Application shutdown library for golang&#34;&gt;death&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;requirements:a3f014005cf18d6d375bffc268e2748e&#34;&gt;Requirements&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Should only need to pass the signals you want to use for shutdown.&lt;/li&gt;
&lt;li&gt;Block the application from shutting down until signal was recieved.&lt;/li&gt;
&lt;li&gt;Optionally pass structs with a Close method to cleanup objects when shutdown is signaled.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-do-you-use-it:a3f014005cf18d6d375bffc268e2748e&#34;&gt;How do you use it?&lt;/h3&gt;

&lt;p&gt;Import death and syscall so that you can pass the signals you want to shutdown with.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
    DEATH &amp;quot;github.com/vrecan/death&amp;quot;
    SYS &amp;quot;syscall&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a death struct with the signals we want to use for shutdown.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    death := DEATH.NewDeath(SYS.SIGINT, SYS.SIGTERM)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once we are doing setting up everything in our application we can then call WaitForDeath() to block our application from shutting down.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    death.WaitForDeath()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also have death automically close structs for you when shutdown is triggered. Create a slice to contain all the items you want to call Close().&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;objects := make([]DEATH.Closable, 0)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your structs just need a Close() method to implement the Closable interface&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type NewType struct {
}

func (n *NewType) Close() {
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once your go routine(s) have been started append them to the slice.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    objects = append(objects, &amp;amp;NewType{})

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now just pass them in when you are going to call WaitForDeath()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    death.WaitForDeath(objects...)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also pass them in one by one if you prefer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;death.WaitForDeath(object1, object2, object3)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;update:a3f014005cf18d6d375bffc268e2748e&#34;&gt;&lt;strong&gt;&lt;em&gt;Update&lt;/em&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The newest version now does closing of objects in parallel with a timeout. This allows you to limit how long you are willing to wait for shutdown of all your objects. Because they are in parallel this is the total time you are willing to wait and not per object.
&lt;a href=&#34;http://vrecan.github.io/post/death_parallel_shutdown/&#34; title=&#34;Application shutdown library for golang&#34;&gt;Parallel shutdown in go&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;summary:a3f014005cf18d6d375bffc268e2748e&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m actually pretty happy with this library. It does everything I need it to do and has been really useful to just drop in and have solid shutdown management. It does currently use seelog to log a few things this could easily be removed. All of our applications use seelog for application logs but I would love to hear if there are any good ideas about managing logging inside of libraries. I&amp;rsquo;ve thought about having the ablilty to pass in a logger with an interface but I would like to hear how other people deal with this.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://github.com/vrecan/death&#34; title=&#34;Application shutdown library for golang&#34;&gt;Managing death in go&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Compiling ZeroMQ for Windows in Centos 7</title>
      <link>http://vrecan.github.io/post/crosscompile_go_zeromq/</link>
      <pubDate>Mon, 02 Mar 2015 20:29:34 -0700</pubDate>
      
      <guid>http://vrecan.github.io/post/crosscompile_go_zeromq/</guid>
      <description>&lt;p&gt;I&#39;ve found it really dificult to find a concrete tutorial for building zeromq for go in windows using MinGW in centos 7. First lets add the epel-release repo.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install epel-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets install mingw64. For this tutorial I am going to just install all the mingw libraries.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install mingw64*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now lets download and compile zeromq using mingw.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://download.zeromq.org/zeromq-4.0.5.zip
unzip zeromq-4.0.5.zip ~/zeromq
cd ~/zeromq
mingw64-configure configure
mingw64-make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets recompile go so that it is also using the mingw compiler.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /usr/local/go/src
sudo  env CGO_ENABLED=1 GOOS=windows GOARCH=amd64 CC_FOR_TARGET=&amp;quot;x86_64-w64-mingw32-gcc&amp;quot; ./make.bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets move to the go project that you are going to be using with zeromq.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd &amp;lt;PROJECT DIR&amp;gt;
env CGO_ENABLED=1 GOOS=windows GOARCH=amd64 CC_FOR_TARGET=&amp;quot;x86_64-w64-mingw32-gcc&amp;quot; CGO_LDFLAGS=&amp;quot;-L/home/&amp;lt;username&amp;gt;/zeromq/src/.libs -static-libgcc&amp;quot; go build -v -x  -o app.exe
minigw_bin=/usr/x86_64-w64-mingw32/sys-root/mingw/bin/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should have created an exe binary that is linked to the mingw dll&#39;s and your recently compiled version of zeromq. Now lets copy the dll&#39;s for the required libraries to the current directory so that we can package this up and put it on a windows box.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp $minigw_bin/libwinpthread-1.dll .
cp $minigw_bin/libgcc_s_seh-1.dll .
cp $minigw_bin/libstdc++-6.dll .
cp ~/zeromq/src/.libs/libzmq.dll .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;p&gt; If you now transfer this folder to a windows(64) 7/8/8.1 box you can now just execute the .exe and have a working go windows application using zeromq.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go Dependency Management</title>
      <link>http://vrecan.github.io/post/godep/</link>
      <pubDate>Sun, 01 Mar 2015 20:38:08 -0700</pubDate>
      
      <guid>http://vrecan.github.io/post/godep/</guid>
      <description>

&lt;p&gt;Godeps is a nice simple tool that allows you to manage dependencies in a very easy way. One major drawback though is that it requires you to commit your dependencies into your repository. This is great for simplicity but horrible for code review. Seeing a commit with 26k files changed really makes it hard to review.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve found a simple solution to the problem if you use the common go import layout&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+-- github.com
|   +-- company
|   |   +-- project
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you follow this layout then you can easily extract all your deps into their own repo like so.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd github.com/company
godep save ./...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will Recursively grab your dependencies that are not in this path and put them in a godeps folder at the company depth. Now add the contents of the new Godeps folder into a new repo with the name Godeps. &lt;/p&gt;

&lt;h2 id=&#34;building-with-new-dependencies:8a18e8b20bd4ceba8c6e1d3df562deeb&#34;&gt;Building with new dependencies&lt;/h2&gt;

&lt;p&gt;Now that you have dependencies in their own repo you can add Godeps to your .gitignore for all your other repos and pull them down locally.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd github.com/company/project
git clone github.com/company/Godeps
godep go buld
godep go test ./... --race
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion-drawbacks:8a18e8b20bd4ceba8c6e1d3df562deeb&#34;&gt;Conclusion / Drawbacks&lt;/h2&gt;

&lt;p&gt;
This works great if you are ok with taking the master branch versions of all your own software. This might not work for everyone but with my workflow this seems to give us the best of both worlds. Easy third party library management and go getable software for all of our repos. This also has the added benefit that we no longer have to remember to update godep everytime we make an internal library change, in our workflow everything in master should be buildable together.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>